---
permalink: /
title: "About"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I’m a Ph.D. researcher at **Nanyang Technological University** working on **trustworthy AI for autonomous & robotic systems**—especially how to *expose, monitor, and manage failure modes* in safety-critical settings. :contentReference[oaicite:1]{index=1}

## Research focus
My work sits at the intersection of:

- **Adversarial scene generation** to systematically surface rare/risky cases
- **Online monitoring** to detect distribution shift and uncertainty at runtime
- **Decision-making under uncertainty** for safer planning and control :contentReference[oaicite:2]{index=2}

## Current theme: “Trustworthy = failure-aware”
Instead of optimizing average-case performance, I focus on **exposing**, **monitoring**, and **mitigating** failure modes. :contentReference[oaicite:3]{index=3}

## Experience (why I care about deployment)
Befo
## An Approach to Trustworthy AI
My research is centered on **adversarial design**. Trustworthy AI is not achieved by average-case performance, but by continuously exposing and managing failure modes. Illustrated here using autonomous driving as a safety-critical testbed, but applicable to broader AI and robotic systems.
<p align="center"> <img src="/images/Philo.jpg"
     alt="Adversarial design philosophy illustration"
     width="600" /> </p>
     
-**Adversary** actively generates rare, risky, and adversarial scenarios to expose system weaknesses, using passive and active testing as well as objective and subjective evaluations. 

-**Adaptor** continuously improves perception, decision-making, and control to mitigate the failures revealed by adversarial exposure. 

-**Collaborator (Human-in-the-loop)** provides human assistance when uncertainty, ambiguity, or risk exceeds the autonomous system’s capability.
